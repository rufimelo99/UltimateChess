default_settings: null
behaviors:
    BehaviorPPO:
        trainer_type: ppo

        hyperparameters:
          # Hyperparameters common to PPO and SAC
          batch_size: 1024
          buffer_size: 10240
          learning_rate: 3.0e-4
          learning_rate_schedule: linear

          # PPO-specific hyperparameters
          # Replaces the "PPO-specific hyperparameters" section above
          beta: 5.0e-3
          epsilon: 0.2
          lambd: 0.95
          num_epoch: 3

        # Configuration of the neural network (common to PPO/SAC)
        network_settings:
          vis_encoder_type: simple
          normalize: false
          hidden_units: 128
          num_layers: 2
          # memory
          memory:
            sequence_length: 64
            memory_size: 256

        # Trainer configurations common to all trainers
        max_steps: 5.0e5
        time_horizon: 64
        summary_freq: 10000
        keep_checkpoints: 5
        checkpoint_interval: 50000
        threaded: true
        init_path: null

        # behavior cloning
        behavioral_cloning:
          demo_path: Project/Assets/ML-Agents/Examples/Pyramids/Demos/ExpertPyramid.demo
          strength: 0.5
          steps: 150000
          batch_size: 512
          num_epoch: 3
          samples_per_update: 0

        reward_signals:
          # environment reward (default)
          extrinsic:
            strength: 1.0
            gamma: 0.99

          # curiosity module
          curiosity:
            strength: 0.02
            gamma: 0.99
            encoding_size: 256
            learning_rate: 3.0e-4

          # GAIL
          gail:
            strength: 0.01
            gamma: 0.99
            encoding_size: 128
            demo_path: Project/Assets/ML-Agents/Examples/Pyramids/Demos/ExpertPyramid.demo
            learning_rate: 3.0e-4
            use_actions: false
            use_vail: false

        # self-play
        self_play:
          window: 10
          play_against_latest_model_ratio: 0.5
          save_steps: 50000
          swap_steps: 2000
          team_change: 100000
env_settings:
  env_path: null
  env_args: null
  base_port: 5005
  num_envs: 1
  seed: -1
engine_settings:
  width: 84
  height: 84
  quality_level: 5
  time_scale: 20
  target_frame_rate: -1
  capture_frame_rate: 60
  no_graphics: false
environment_parameters: null
checkpoint_settings:
  run_id: ppo
  initialize_from: null
  load_model: false
  resume: true
  force: false
  train_model: false
  inference: false
debug: false
